{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "epXULuw_s9LB"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "from collections import deque\n",
    "\n",
    "from skimage import io, color, transform\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "'''Pick your poison: Keras, Pytorch, Tensorflow'''\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "GAME_TYPE = 'MsPacman-v0'\n",
    "env =gym.make(\"MsPacman-v0\")   # USE this env: MsPacman-v0\n",
    "# import your desired environment, might need to download some dependancies for atari environments. Prepend ! to use cli commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "forM1Dmos9LZ",
    "outputId": "a4f29b4d-4a76-4709-9ac6-33b0934efa74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "#####\n",
    "# Hyperparameters\n",
    "#####\n",
    "\n",
    "''' These are not the golden hyperparameters, just put there to give a rough idea of magnitude. Search for parameters in this ball park'''\n",
    "#environment parameters\n",
    "NUM_EPISODES = 80000000\n",
    "PHI_LENGTH = 4\n",
    "\n",
    "#agent parameters\n",
    "EPSILON = 1\n",
    "EXPERIENCE_REPLAY_CAPACITY = 2000\n",
    "MINIBATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.001\n",
    "ACTION_SIZE = env.action_space.n\n",
    "EXPLORE = 3000000\n",
    "PREPROCESS_IMAGE_DIM = 84\n",
    "\n",
    "print(ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "giYOD1qBzDJk"
   },
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxUV8tl5ce-a"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.processed_image_dim = PREPROCESS_IMAGE_DIM \n",
    "        \n",
    "    def preprocess_observation(self, observation, prediction=False):\n",
    "        \"\"\"\n",
    "        Helper function for preprocessing an observation for consumption by our\n",
    "        deep learning network\n",
    "        \"\"\"\n",
    "        grayscale_observation = color.rgb2gray(observation)\n",
    "        resized_observation = transform.resize(grayscale_observation, (1, self.processed_image_dim, self.processed_image_dim)).astype('float32')\n",
    "        if prediction:\n",
    "            resized_observation = np.expand_dims(resized_observation, 0)\n",
    "        return resized_observation\n",
    "      \n",
    "obj = Agent() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "SDQdd2A5xcf4",
    "outputId": "013c9797-69b2-4238-c220-a3cba433dc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State.shape = (210, 160, 3)\n",
      "State_size = 100800\n",
      "State.shape now = (1, 100800)\n",
      "State_size now = 100800\n",
      "env.observation_space.shape = (210, 160, 3)\n",
      "next_State.shape = (210, 160, 3)\n",
      "next_State.shape now = (1, 100800)\n",
      "s_t.shape = (1, 84, 84, 4)\n",
      "(210, 160, 3)\n",
      "x_t1.shape = (1, 84, 84)\n",
      "x_t1.shape now = (1, 84, 84, 1)\n",
      "s_t1.shape = (1, 84, 84, 4)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use this cell just to explore the environment, not used towards the algo\n",
    "'''\n",
    "state =env.reset()\n",
    "#print state\n",
    "print(\"State.shape = %s\" % (state.shape,))\n",
    "print (\"State_size = %s\" % (state.size,))\n",
    "#laying out all features in 1 row, for fun\n",
    "state = np.reshape(state, [1, state.size])\n",
    "#print state\n",
    "print (\"State.shape now = %s\" % (state.shape,))\n",
    "print (\"State_size now = %s\" % (state.size,))\n",
    "#print env.observation_space\n",
    "print (\"env.observation_space.shape = %s\" %(env.observation_space.shape,))\n",
    "\n",
    "action = random.randrange(env.action_space.n)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "#print next_state\n",
    "print (\"next_State.shape = %s\" % (next_state.shape,))\n",
    "next_state = np.reshape(state, [1, next_state.size])\n",
    "#print next_state\n",
    "print (\"next_State.shape now = %s\" % (next_state.shape,))\n",
    "\n",
    "#slight change of notations\n",
    "#ok let next_state be called x_t1 and current state be x_t\n",
    "# let reward be r_t and action a_t, 9=env.action_space.n= possible actions\n",
    "\n",
    "\n",
    "x_t = env.reset()\n",
    "x_t = obj.preprocess_observation(x_t)\n",
    "    #print (\"x_t.shape = %s\"% (x_t.shape,))\n",
    "\n",
    "\n",
    "s_t = np.stack((x_t, x_t, x_t, x_t), axis=3)\n",
    "print (\"s_t.shape = %s\"% (s_t.shape,))\n",
    "\n",
    "    #above stuff - one time per episode at the start eventualy set s_t = s_t1 and so on\n",
    "\n",
    "a_t = random.randrange(env.action_space.n)\n",
    "x_t1 , r_t, done, info = env.step(a_t)\n",
    "print (x_t1.shape)\n",
    "x_t1= obj.preprocess_observation(x_t1)\n",
    "print (\"x_t1.shape = %s\"% (x_t1.shape,))\n",
    "x_t1 = x_t1.reshape(x_t1.shape[0], x_t1.shape[1], x_t1.shape[2],1) #1x84x84x1\n",
    "print (\"x_t1.shape now = %s\"% (x_t1.shape,))\n",
    "s_t1 = np.append(x_t1, s_t[:, :, :, :3], axis=3)\n",
    "print (\"s_t1.shape = %s\"% (s_t1.shape,))\n",
    "        #s_t = s_t1\n",
    "    #now s_t =s_t1 and then loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ADuU3tfBs9L3",
    "outputId": "731b0733-cb74-47a5-c402-d804ae68fd33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([(array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32), 5, 0.0, array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]], dtype=float32), False), (array([[[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]]], dtype=float32), 5, 0.0, array([[[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]]], dtype=float32), False)], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "#visualising deque\n",
    "D = deque(maxlen=5)\n",
    "D.append((x_t, a_t, r_t, x_t1, done))\n",
    "D.append((s_t, a_t, r_t, s_t1, done))\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yH3aQLqMzL20"
   },
   "source": [
    "#Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyPvqrLos9MC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lets begin\n",
    "UPDATE_EVERY = 5\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, epsilon , experience_replay_capacity , minibatch_size , learning_rate ,action_size, img_dim , explore):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = 0.01\n",
    "        self.batch_size = minibatch_size\n",
    "        self.train_start = 1000\n",
    "        self.explore = explore\n",
    "        self.img_channels = 4 #phi_length  #coz we feed in 4 stacked b&w imgs instead of 1 rbg img\n",
    "        self.processed_image_dim = img_dim\n",
    "        \n",
    "         # create replay memory using deque\n",
    "        self.D = deque(maxlen=experience_replay_capacity)\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "        # initialize target model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def build_model(self) :\n",
    "        '''Write your deep learning model here eg. CNN or RNN etc'''\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8,8), padding='same', strides=(4,4), input_shape=(84, 84, 4)))\n",
    "        model.add(Conv2D(64, (4,4), padding='same', strides=(2,2)))\n",
    "        model.add(Conv2D(64, (3,3), padding='same', strides=(1,1)))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Dense(ACTION_SIZE, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr = LEARNING_RATE), metrics=['accuracy'])\n",
    "        #try printing model summary\n",
    "        print(model.summary())\n",
    "        #FILL THIS\n",
    "        print(\"finish building the model\")\n",
    "        return model\n",
    "      \n",
    "    # after some time interval update the target model to be same with model ###Q-learning specific function\n",
    "    '''def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        #FILL '''\n",
    "        \n",
    "    def append_experience_replay_example(self,s_t,a_t,r_t,s_t1,done):\n",
    "        \"\"\"\n",
    "        Add an experience replay example to our agent's replay memory. If\n",
    "        memory is full, overwrite previous examples, starting with the oldest\n",
    "        \"\"\"\n",
    "        D.append((s_t, a_t, r_t, s_t1, done))\n",
    "       \n",
    "        if self.epsilon > self.epsilon_min :\n",
    "            self.epsilon -= (self.epsilon - self.epsilon_min) /self.explore\n",
    "            #this is just one way of reducing exploration, try out other ways, experiment\n",
    "        \n",
    "    def preprocess_observation(self, observation, prediction=False):\n",
    "        \"\"\"\n",
    "        Helper function for preprocessing an observation for consumption by our\n",
    "        deep learning network\n",
    "        \"\"\"\n",
    "        grayscale_observation = color.rgb2gray(observation)\n",
    "        resized_observation = transform.resize(grayscale_observation, (1, self.processed_image_dim, self.processed_image_dim)).astype('float32')\n",
    "        if prediction:\n",
    "            resized_observation = np.expand_dims(resized_observation, 0)\n",
    "        return resized_observation\n",
    "        \n",
    "    def take_action(self, s_t):\n",
    "        \"\"\"\n",
    "        Given an observation, the model attempts to take an action\n",
    "        according to its q-function approximation\n",
    "        \"\"\"\n",
    "        k = self.model.predict(np.array(s_t).reshape(-1, *s_t.shape))[0]\n",
    "        action = np.argmax(k)\n",
    "        return action\n",
    "        #FILL THIS\n",
    "        \n",
    "    def learn(self, terminal_state, step):\n",
    "        \"\"\"\n",
    "        Allow the model to collect examples from its experience replay memory\n",
    "        and learn from them\n",
    "        \"\"\"\n",
    "        #this is similar to your train loop or model.fit for those not comfortable with RL\n",
    "        #FILL THIS\n",
    "        if len(self.D)<MIN_REPLAY_CAPACITY:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.D, minibatch_size)\n",
    "        \n",
    "        #get the q vals\n",
    "        current_states = np.array([s_t for s_t in minibatch_size])\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "        \n",
    "        new_current_states = np.array([s_t1 for s_t1 in minibatch_size])\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for index, (s_t, a_t, r_t, s_t1, done) in enumerate(minibatch_size):\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = r_t + discount_factor * max_future_q\n",
    "            else:\n",
    "                new_q = r_t\n",
    "                \n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_qs\n",
    "            \n",
    "            X.append(s_t)\n",
    "            y.append(cuurent_qs)\n",
    "            \n",
    "        self.model.fit(np.array(X), np.array(y), verbose=0, batch_size=minibatch_size, shuffle=False)\n",
    "        if terminal_state:\n",
    "            self.target_update_counter+=1\n",
    "        \n",
    "        if self.target.update_counter>UPDATE_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SCH-klks9MI"
   },
   "outputs": [],
   "source": [
    "MIN_REPLAY_CAPACITY = 5\n",
    "step = 0\n",
    "def run_simulation():\n",
    "    \"\"\"\n",
    "    Entry-point for running env simulation\n",
    "    \"\"\"\n",
    "\n",
    "    #print game parameters\n",
    "    print (\"~~~Environment Parameters~~~\")\n",
    "    print (\"Num episodes: %s\" % NUM_EPISODES)\n",
    "    print (\"Action space: %s\" % env.action_space)\n",
    "    print()\n",
    "    print (\"~~~Agent Parameters~~~\")\n",
    "    print (\"Epsilon: %s\" % EPSILON)\n",
    "    print (\"Experience Replay Capacity: %s\" % EXPERIENCE_REPLAY_CAPACITY)\n",
    "    print (\"Minibatch Size: %s\" % MINIBATCH_SIZE)\n",
    "    print (\"Learning Rate: %s\" % LEARNING_RATE)\n",
    "\n",
    "    #initialize agent\n",
    "    agent = Agent(epsilon=EPSILON,\n",
    "                experience_replay_capacity=EXPERIENCE_REPLAY_CAPACITY,\n",
    "                minibatch_size=MINIBATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE, action_size =ACTION_SIZE, img_dim =PREPROCESS_IMAGE_DIM ,explore =EXPLORE)\n",
    "    \n",
    "    scores, episodes = [], [] \n",
    "\n",
    "    #initialize auxiliary data structures\n",
    "    state_list = [] \n",
    "    tot_frames = 0\n",
    "\n",
    "    for i_episode in range(NUM_EPISODES):\n",
    "        print (\"Episode: %s\" % i_episode)\n",
    "         \n",
    "        done = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        x_t = env.reset()\n",
    "        x_t = agent.preprocess_observation(x_t)   \n",
    "        s_t = np.stack((x_t, x_t, x_t, x_t), axis=3) #how many consecutive frames to stack depends on your PHI\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "        while not done:\n",
    "            #env.render()\n",
    "           # get action for the current state and go one step in environment\n",
    "           # get action, change score and learn from memory\n",
    "           #FILL THIS\n",
    "            if np.random.random()>EPSILON:\n",
    "                a_t = agent.take_action(s_t)\n",
    "            else:\n",
    "                a_t = env.action_space.sample()\n",
    "                \n",
    "            s_t1, r_t, done, _ = env.step(action)\n",
    "            score += r_t\n",
    "            agent.append_experience_replay_example(s_t, a_t, r_t, s_t1, done)\n",
    "            agent.learn(done, step)\n",
    "            step+=1\n",
    "            s_t = s_t1\n",
    "            \n",
    "        if done:\n",
    "            # every episode update the target model to be same with model\n",
    "            #agent.update_target_model() \n",
    "            scores.append(score)\n",
    "            episodes.append(i_episode)\n",
    "            print( \"  score:\", score, \"  epsilon:\", agent.epsilon)\n",
    "            \n",
    "        while True:\n",
    "           \n",
    "            #ensure state list is populated\n",
    "            if tot_frames < PHI_LENGTH:\n",
    "                state_list.append(agent.preprocess_observation(x_t))\n",
    "                tot_frames += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "                #update state list with next observation\n",
    "                state_list.append(preprocess_observation(x_t))\n",
    "                state_list.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrBjXxDCs9MR",
    "outputId": "31e8c0c2-5039-47b8-dab4-765cbadb398a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~Environment Parameters~~~\n",
      "Num episodes: 80000000\n",
      "Action space: Discrete(9)\n",
      "\n",
      "~~~Agent Parameters~~~\n",
      "Epsilon: 1\n",
      "Experience Replay Capacity: 2000\n",
      "Minibatch Size: 100\n",
      "Learning Rate: 0.001\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 21, 21, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 11, 11, 64)        32832     \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               991360    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 1,070,505\n",
      "Trainable params: 1,070,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "finish building the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 21, 21, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 11, 11, 64)        32832     \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 7744)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               991360    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 1,070,505\n",
      "Trainable params: 1,070,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "finish building the model\n",
      "Episode: 0\n",
      "  score: 90.0   epsilon: 0.9997828837750969\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-4519ca11f186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MsPacman-v0\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#env name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-101-fb531a90a444>\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m#ensure state list is populated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtot_frames\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mPHI_LENGTH\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mstate_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mtot_frames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env =gym.make(\"MsPacman-v0\") #env name\n",
    "    run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Starter_code_RL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
